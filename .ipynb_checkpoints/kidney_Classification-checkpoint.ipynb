{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "from scipy.stats import sem, t\n",
    "from scipy import mean\n",
    "\n",
    "#Oversampling\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(922, 33)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>SBP</th>\n",
       "      <th>BMI</th>\n",
       "      <th>etiology of CKD</th>\n",
       "      <th>Hb</th>\n",
       "      <th>Alb</th>\n",
       "      <th>Cr</th>\n",
       "      <th>eGFR</th>\n",
       "      <th>CKD_stage</th>\n",
       "      <th>...</th>\n",
       "      <th>eGFR(6M)</th>\n",
       "      <th>eGFR(12M)</th>\n",
       "      <th>eGFR(18M)</th>\n",
       "      <th>eGFR(24M)</th>\n",
       "      <th>eGFR(30M)</th>\n",
       "      <th>eGFR(36M)</th>\n",
       "      <th>eGFR(last visit)</th>\n",
       "      <th>average_obs</th>\n",
       "      <th>obsevasion_ duration</th>\n",
       "      <th>fclass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>74</td>\n",
       "      <td>120.0</td>\n",
       "      <td>23.137669</td>\n",
       "      <td>2</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.20</td>\n",
       "      <td>89.981926</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>26.454698</td>\n",
       "      <td>24.331582</td>\n",
       "      <td>24.682189</td>\n",
       "      <td>21.614854</td>\n",
       "      <td>20.420524</td>\n",
       "      <td>20.420524</td>\n",
       "      <td>18.495328</td>\n",
       "      <td>25.275139</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>139.0</td>\n",
       "      <td>28.515625</td>\n",
       "      <td>2</td>\n",
       "      <td>15.9</td>\n",
       "      <td>4.8</td>\n",
       "      <td>0.84</td>\n",
       "      <td>88.330020</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>78.287758</td>\n",
       "      <td>71.343858</td>\n",
       "      <td>72.845992</td>\n",
       "      <td>71.908942</td>\n",
       "      <td>71.562914</td>\n",
       "      <td>67.225032</td>\n",
       "      <td>67.225032</td>\n",
       "      <td>72.392152</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>154.0</td>\n",
       "      <td>24.582701</td>\n",
       "      <td>4</td>\n",
       "      <td>14.4</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0.87</td>\n",
       "      <td>86.973875</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>75.027238</td>\n",
       "      <td>69.595257</td>\n",
       "      <td>68.856399</td>\n",
       "      <td>72.901926</td>\n",
       "      <td>69.749275</td>\n",
       "      <td>69.171408</td>\n",
       "      <td>69.171408</td>\n",
       "      <td>72.694258</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>144.0</td>\n",
       "      <td>30.737407</td>\n",
       "      <td>2</td>\n",
       "      <td>14.4</td>\n",
       "      <td>4.7</td>\n",
       "      <td>2.22</td>\n",
       "      <td>86.874201</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>26.885061</td>\n",
       "      <td>24.917353</td>\n",
       "      <td>28.581660</td>\n",
       "      <td>29.237135</td>\n",
       "      <td>25.556002</td>\n",
       "      <td>25.183703</td>\n",
       "      <td>25.183703</td>\n",
       "      <td>26.485251</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>135.0</td>\n",
       "      <td>23.758726</td>\n",
       "      <td>2</td>\n",
       "      <td>17.0</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.39</td>\n",
       "      <td>86.782629</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>46.978867</td>\n",
       "      <td>45.829455</td>\n",
       "      <td>41.488436</td>\n",
       "      <td>41.801561</td>\n",
       "      <td>38.106104</td>\n",
       "      <td>38.106104</td>\n",
       "      <td>38.106104</td>\n",
       "      <td>43.081581</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender  age    SBP        BMI  etiology of CKD    Hb  Alb    Cr       eGFR  \\\n",
       "0       2   74  120.0  23.137669                2  12.0  4.0  1.20  89.981926   \n",
       "1       1   57  139.0  28.515625                2  15.9  4.8  0.84  88.330020   \n",
       "2       1   32  154.0  24.582701                4  14.4  4.4  0.87  86.973875   \n",
       "3       1   60  144.0  30.737407                2  14.4  4.7  2.22  86.874201   \n",
       "4       1   49  135.0  23.758726                2  17.0  4.1  1.39  86.782629   \n",
       "\n",
       "   CKD_stage  ...   eGFR(6M)  eGFR(12M)  eGFR(18M)  eGFR(24M)  eGFR(30M)  \\\n",
       "0          3  ...  26.454698  24.331582  24.682189  21.614854  20.420524   \n",
       "1          2  ...  78.287758  71.343858  72.845992  71.908942  71.562914   \n",
       "2          2  ...  75.027238  69.595257  68.856399  72.901926  69.749275   \n",
       "3          4  ...  26.885061  24.917353  28.581660  29.237135  25.556002   \n",
       "4          3  ...  46.978867  45.829455  41.488436  41.801561  38.106104   \n",
       "\n",
       "   eGFR(36M)  eGFR(last visit)  average_obs  obsevasion_ duration  fclass  \n",
       "0  20.420524         18.495328    25.275139                    37       0  \n",
       "1  67.225032         67.225032    72.392152                    37       0  \n",
       "2  69.171408         69.171408    72.694258                    36       0  \n",
       "3  25.183703         25.183703    26.485251                    35       0  \n",
       "4  38.106104         38.106104    43.081581                    30       0  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('prepared Data.csv')\n",
    "print(data.shape)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 611, 1: 170, 5: 51, 2: 42, 6: 48})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = Counter(data['fclass'])\n",
    "count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define oversampling strategy\n",
    "oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "\n",
    "## Below code works for binary classification \n",
    "# oversample = RandomOverSampler(sampling_strategy=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # define oversampling strategy\n",
    "# oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "\n",
    "# # fit and apply the transform\n",
    "# X_over, y_over = oversample.fit_resample(data.values[:,:-1], data.values[:,-1])\n",
    "\n",
    "# Max = dict(Counter(y_over))[0.0]\n",
    "# flag = True\n",
    "# while flag:\n",
    "#     X_over, y_over = oversample.fit_resample(X_over, y_over)\n",
    "#     for x in list(dict(Counter(y_over)).values()):\n",
    "#         if x == Max:\n",
    "#             flag = False\n",
    "#         else:\n",
    "#             flag = True\n",
    "#             break\n",
    "        \n",
    "# # summarize class distribution\n",
    "# print(Counter(y_over))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # example of evaluating a decision tree with random oversampling\n",
    "# from numpy import mean\n",
    "# from sklearn.datasets import make_classification\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from imblearn.pipeline import Pipeline\n",
    "# from imblearn.over_sampling import RandomOverSampler\n",
    "# # define dataset\n",
    "# X, y = make_classification(n_samples=10000, weights=[0.99], flip_y=0)\n",
    "# # define pipeline\n",
    "# steps = [('over', RandomOverSampler()), ('model', DecisionTreeClassifier())]\n",
    "# pipeline = Pipeline(steps=steps)\n",
    "# # evaluate pipeline\n",
    "# cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# scores = cross_val_score(pipeline, X_over, y_over, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# score = mean(scores)\n",
    "# print('F1 Score: %.3f' % score)\n",
    "# scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adict = dict()\n",
    "# adict.update(zip(d.columns,col[:-1]))\n",
    "# adict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = pd.DataFrame(X_over)\n",
    "# adict = dict()\n",
    "# adict.update(zip(d.columns,col[:-1]))\n",
    "# d.rename(columns = adict, inplace =True)\n",
    "# d['fclass'] = y_over\n",
    "# d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 611, 1: 170, 5: 51, 2: 42, 6: 48})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = Counter(data['fclass'])\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = d.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat0 = shuffle(data[data['fclass']==0])\n",
    "cat1 = shuffle(data[data['fclass']==1])\n",
    "cat2 = shuffle(data[data['fclass']==2])\n",
    "cat5 = shuffle(data[data['fclass']==5])\n",
    "cat6 = shuffle(data[data['fclass']==6])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lenCat0 = len(cat0.iloc[:,0])\n",
    "lenCat1 = len(cat1.iloc[:,0])\n",
    "lenCat2 = len(cat2.iloc[:,0])\n",
    "lenCat5 = len(cat5.iloc[:,0])\n",
    "lenCat6 = len(cat6.iloc[:,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "methodList = []\n",
    "accRes = []\n",
    "accConf = []\n",
    "# sensitivityList = []\n",
    "# specificityList = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = ['Logistic Regression', 'K Neighbors Classifier', 'Random Forest', 'Gaussian Naive Bayes',\\\n",
    "          'Linear Discriminant Analysis', 'Decision Tree', 'Support Vector Machine', 'Gradient Boosting',\\\n",
    "          'VotingClassifier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(831, 32) (831,)\n",
      "Logistic Regression [[60  1  0  0  0]\n",
      " [ 0 17  0  0  0]\n",
      " [ 3  1  0  0  0]\n",
      " [ 5  0  0  0  0]\n",
      " [ 1  2  0  0  1]]\n",
      "(830, 32) (830,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amin/.local/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/amin/.local/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression [[58  2  1  0  0]\n",
      " [ 0 14  0  0  3]\n",
      " [ 3  0  1  0  0]\n",
      " [ 4  1  0  0  0]\n",
      " [ 0  5  0  0  0]]\n",
      "(830, 32) (830,)\n",
      "Logistic Regression [[59  0  1  0  1]\n",
      " [ 2 15  0  0  0]\n",
      " [ 2  1  1  0  0]\n",
      " [ 4  1  0  0  0]\n",
      " [ 0  4  1  0  0]]\n",
      "(830, 32) (830,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amin/.local/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/amin/.local/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression [[56  4  1  0  0]\n",
      " [ 3 14  0  0  0]\n",
      " [ 4  0  0  0  0]\n",
      " [ 4  0  1  0  0]\n",
      " [ 0  2  0  0  3]]\n",
      "(829, 32) (829,)\n",
      "Logistic Regression [[61  0  0  0  0]\n",
      " [ 0 17  0  0  0]\n",
      " [ 4  1  0  0  0]\n",
      " [ 5  0  0  0  0]\n",
      " [ 1  4  0  0  0]]\n",
      "(831, 32) (831,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amin/.local/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/amin/.local/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression [[61  0  0  0  0]\n",
      " [ 0 17  0  0  0]\n",
      " [ 2  2  0  0  0]\n",
      " [ 5  0  0  0  0]\n",
      " [ 0  3  0  0  1]]\n",
      "(830, 32) (830,)\n",
      "Logistic Regression [[61  0  0  0  0]\n",
      " [ 2 13  0  0  2]\n",
      " [ 3  0  1  0  0]\n",
      " [ 5  0  0  0  0]\n",
      " [ 0  2  1  0  2]]\n",
      "(830, 32) (830,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amin/.local/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/amin/.local/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression [[56  4  1  0  0]\n",
      " [ 2 12  2  0  1]\n",
      " [ 2  2  0  0  0]\n",
      " [ 5  0  0  0  0]\n",
      " [ 0  5  0  0  0]]\n",
      "(830, 32) (830,)\n",
      "Logistic Regression [[61  0  0  0  0]\n",
      " [ 1 13  1  0  2]\n",
      " [ 2  1  1  0  0]\n",
      " [ 5  0  0  0  0]\n",
      " [ 0  4  0  0  1]]\n",
      "(827, 32) (827,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amin/.local/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/amin/.local/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression [[61  0  0  0  1]\n",
      " [ 2 14  0  0  1]\n",
      " [ 3  1  1  0  0]\n",
      " [ 6  0  0  0  0]\n",
      " [ 2  2  1  0  0]]\n",
      "Accuracy:         0.82  -+0.0268\n",
      "(831, 32) (831,)\n",
      "K Neighbors Classifier [[60  1  0  0  0]\n",
      " [ 8  9  0  0  0]\n",
      " [ 3  0  0  0  1]\n",
      " [ 5  0  0  0  0]\n",
      " [ 0  3  0  0  1]]\n",
      "(830, 32) (830,)\n",
      "K Neighbors Classifier [[57  3  0  1  0]\n",
      " [ 1 16  0  0  0]\n",
      " [ 3  1  0  0  0]\n",
      " [ 4  1  0  0  0]\n",
      " [ 1  3  0  0  1]]\n",
      "(830, 32) (830,)\n",
      "K Neighbors Classifier [[59  2  0  0  0]\n",
      " [ 6 11  0  0  0]\n",
      " [ 4  0  0  0  0]\n",
      " [ 5  0  0  0  0]\n",
      " [ 1  4  0  0  0]]\n",
      "(830, 32) (830,)\n",
      "K Neighbors Classifier [[53  6  1  1  0]\n",
      " [ 4 13  0  0  0]\n",
      " [ 3  1  0  0  0]\n",
      " [ 4  0  1  0  0]\n",
      " [ 1  4  0  0  0]]\n",
      "(829, 32) (829,)\n",
      "K Neighbors Classifier [[60  1  0  0  0]\n",
      " [ 2 15  0  0  0]\n",
      " [ 4  1  0  0  0]\n",
      " [ 4  1  0  0  0]\n",
      " [ 0  5  0  0  0]]\n",
      "(831, 32) (831,)\n",
      "K Neighbors Classifier [[58  3  0  0  0]\n",
      " [ 4 12  1  0  0]\n",
      " [ 1  2  1  0  0]\n",
      " [ 5  0  0  0  0]\n",
      " [ 1  3  0  0  0]]\n",
      "(830, 32) (830,)\n",
      "K Neighbors Classifier [[61  0  0  0  0]\n",
      " [ 2 15  0  0  0]\n",
      " [ 4  0  0  0  0]\n",
      " [ 5  0  0  0  0]\n",
      " [ 1  4  0  0  0]]\n",
      "(830, 32) (830,)\n",
      "K Neighbors Classifier [[54  7  0  0  0]\n",
      " [ 2 15  0  0  0]\n",
      " [ 2  2  0  0  0]\n",
      " [ 5  0  0  0  0]\n",
      " [ 1  4  0  0  0]]\n",
      "(830, 32) (830,)\n",
      "K Neighbors Classifier [[61  0  0  0  0]\n",
      " [ 3 14  0  0  0]\n",
      " [ 4  0  0  0  0]\n",
      " [ 5  0  0  0  0]\n",
      " [ 0  5  0  0  0]]\n",
      "(827, 32) (827,)\n",
      "K Neighbors Classifier [[59  3  0  0  0]\n",
      " [ 6 11  0  0  0]\n",
      " [ 5  0  0  0  0]\n",
      " [ 5  0  1  0  0]\n",
      " [ 2  3  0  0  0]]\n",
      "Accuracy:         0.78  -+0.0258\n",
      "(831, 32) (831,)\n",
      "Random Forest [[60  1  0  0  0]\n",
      " [ 3 14  0  0  0]\n",
      " [ 3  1  0  0  0]\n",
      " [ 5  0  0  0  0]\n",
      " [ 1  3  0  0  0]]\n",
      "(830, 32) (830,)\n",
      "Random Forest [[59  2  0  0  0]\n",
      " [ 0 17  0  0  0]\n",
      " [ 3  1  0  0  0]\n",
      " [ 4  1  0  0  0]\n",
      " [ 0  5  0  0  0]]\n",
      "(830, 32) (830,)\n",
      "Random Forest [[59  1  0  0  1]\n",
      " [ 2 14  0  0  1]\n",
      " [ 3  0  1  0  0]\n",
      " [ 5  0  0  0  0]\n",
      " [ 0  5  0  0  0]]\n",
      "(830, 32) (830,)\n",
      "Random Forest [[58  3  0  0  0]\n",
      " [ 2 14  0  0  1]\n",
      " [ 3  1  0  0  0]\n",
      " [ 5  0  0  0  0]\n",
      " [ 0  5  0  0  0]]\n",
      "(829, 32) (829,)\n",
      "Random Forest [[60  1  0  0  0]\n",
      " [ 0 17  0  0  0]\n",
      " [ 4  1  0  0  0]\n",
      " [ 5  0  0  0  0]\n",
      " [ 0  5  0  0  0]]\n",
      "(831, 32) (831,)\n",
      "Random Forest [[59  2  0  0  0]\n",
      " [ 2 13  1  0  1]\n",
      " [ 1  3  0  0  0]\n",
      " [ 5  0  0  0  0]\n",
      " [ 0  3  1  0  0]]\n",
      "(830, 32) (830,)\n",
      "Random Forest [[61  0  0  0  0]\n",
      " [ 1 16  0  0  0]\n",
      " [ 4  0  0  0  0]\n",
      " [ 5  0  0  0  0]\n",
      " [ 1  4  0  0  0]]\n",
      "(830, 32) (830,)\n",
      "Random Forest [[56  5  0  0  0]\n",
      " [ 1 16  0  0  0]\n",
      " [ 2  2  0  0  0]\n",
      " [ 5  0  0  0  0]\n",
      " [ 0  5  0  0  0]]\n",
      "(830, 32) (830,)\n",
      "Random Forest [[61  0  0  0  0]\n",
      " [ 2 15  0  0  0]\n",
      " [ 3  1  0  0  0]\n",
      " [ 5  0  0  0  0]\n",
      " [ 0  5  0  0  0]]\n",
      "(827, 32) (827,)\n",
      "Random Forest [[61  1  0  0  0]\n",
      " [ 3 14  0  0  0]\n",
      " [ 5  0  0  0  0]\n",
      " [ 6  0  0  0  0]\n",
      " [ 0  4  0  0  1]]\n",
      "Accuracy:         0.81  -+0.0142\n",
      "(831, 32) (831,)\n",
      "Gaussian Naive Bayes [[29  5  3 22  2]\n",
      " [ 4  3  0  0 10]\n",
      " [ 0  0  2  1  1]\n",
      " [ 2  0  1  2  0]\n",
      " [ 0  0  0  0  4]]\n",
      "(830, 32) (830,)\n",
      "Gaussian Naive Bayes [[36  3  6 10  6]\n",
      " [ 0  3  0  1 13]\n",
      " [ 0  1  2  1  0]\n",
      " [ 0  1  1  2  1]\n",
      " [ 0  0  0  0  5]]\n",
      "(830, 32) (830,)\n",
      "Gaussian Naive Bayes [[33  3  2 20  3]\n",
      " [ 0  3  0  1 13]\n",
      " [ 0  2  1  1  0]\n",
      " [ 2  1  0  2  0]\n",
      " [ 0  0  0  0  5]]\n",
      "(830, 32) (830,)\n",
      "Gaussian Naive Bayes [[33  5  4 12  7]\n",
      " [ 3  5  0  0  9]\n",
      " [ 2  0  1  0  1]\n",
      " [ 2  0  2  1  0]\n",
      " [ 0  0  0  0  5]]\n",
      "(829, 32) (829,)\n",
      "Gaussian Naive Bayes [[33  1  4 19  4]\n",
      " [ 0  2  1  1 13]\n",
      " [ 1  3  1  0  0]\n",
      " [ 3  1  0  1  0]\n",
      " [ 0  1  0  0  4]]\n",
      "(831, 32) (831,)\n",
      "Gaussian Naive Bayes [[32  5  4 16  4]\n",
      " [ 0  7  0  1  9]\n",
      " [ 0  0  1  0  3]\n",
      " [ 1  0  0  4  0]\n",
      " [ 0  1  0  0  3]]\n",
      "(830, 32) (830,)\n",
      "Gaussian Naive Bayes [[38  7  1 15  0]\n",
      " [ 0  2  0  1 14]\n",
      " [ 1  0  3  0  0]\n",
      " [ 2  0  0  3  0]\n",
      " [ 0  1  0  0  4]]\n",
      "(830, 32) (830,)\n",
      "Gaussian Naive Bayes [[39  5  3  7  7]\n",
      " [ 0  4  0  0 13]\n",
      " [ 1  0  0  1  2]\n",
      " [ 4  0  1  0  0]\n",
      " [ 0  0  0  0  5]]\n",
      "(830, 32) (830,)\n",
      "Gaussian Naive Bayes [[33  7  7 14  0]\n",
      " [ 0  4  0  1 12]\n",
      " [ 0  0  1  1  2]\n",
      " [ 2  1  0  2  0]\n",
      " [ 0  2  0  0  3]]\n",
      "(827, 32) (827,)\n",
      "Gaussian Naive Bayes [[31  5  6 14  6]\n",
      " [ 1  0  2  2 12]\n",
      " [ 1  1  0  2  1]\n",
      " [ 1  0  2  3  0]\n",
      " [ 0  1  0  0  4]]\n",
      "Accuracy:         0.48  -+0.0325\n",
      "(831, 32) (831,)\n",
      "Linear Discriminant Analysis [[58  2  0  1  0]\n",
      " [ 4 12  1  0  0]\n",
      " [ 3  1  0  0  0]\n",
      " [ 5  0  0  0  0]\n",
      " [ 1  2  0  0  1]]\n",
      "(830, 32) (830,)\n",
      "Linear Discriminant Analysis [[57  2  2  0  0]\n",
      " [ 2 12  0  0  3]\n",
      " [ 1  1  1  0  1]\n",
      " [ 4  1  0  0  0]\n",
      " [ 1  4  0  0  0]]\n",
      "(830, 32) (830,)\n",
      "Linear Discriminant Analysis [[58  1  0  0  2]\n",
      " [ 3 12  0  0  2]\n",
      " [ 3  0  1  0  0]\n",
      " [ 5  0  0  0  0]\n",
      " [ 0  5  0  0  0]]\n",
      "(830, 32) (830,)\n",
      "Linear Discriminant Analysis [[54  4  1  1  1]\n",
      " [ 3 11  1  0  2]\n",
      " [ 3  1  0  0  0]\n",
      " [ 3  0  1  1  0]\n",
      " [ 0  3  0  0  2]]\n",
      "(829, 32) (829,)\n",
      "Linear Discriminant Analysis [[58  1  2  0  0]\n",
      " [ 3 11  1  0  2]\n",
      " [ 2  1  1  0  1]\n",
      " [ 4  0  1  0  0]\n",
      " [ 1  1  1  0  2]]\n",
      "(831, 32) (831,)\n",
      "Linear Discriminant Analysis [[59  1  1  0  0]\n",
      " [ 6  9  2  0  0]\n",
      " [ 0  3  1  0  0]\n",
      " [ 5  0  0  0  0]\n",
      " [ 0  3  0  0  1]]\n",
      "(830, 32) (830,)\n",
      "Linear Discriminant Analysis [[61  0  0  0  0]\n",
      " [ 2 10  0  0  5]\n",
      " [ 3  0  1  0  0]\n",
      " [ 5  0  0  0  0]\n",
      " [ 0  3  1  0  1]]\n",
      "(830, 32) (830,)\n",
      "Linear Discriminant Analysis [[54  5  1  0  1]\n",
      " [ 3 11  1  0  2]\n",
      " [ 2  1  0  1  0]\n",
      " [ 5  0  0  0  0]\n",
      " [ 0  2  1  0  2]]\n",
      "(830, 32) (830,)\n",
      "Linear Discriminant Analysis [[60  0  0  1  0]\n",
      " [ 2 11  0  0  4]\n",
      " [ 2  0  1  0  1]\n",
      " [ 4  0  1  0  0]\n",
      " [ 1  3  0  0  1]]\n",
      "(827, 32) (827,)\n",
      "Linear Discriminant Analysis [[54  2  0  1  5]\n",
      " [ 5 11  1  0  0]\n",
      " [ 4  0  1  0  0]\n",
      " [ 5  0  1  0  0]\n",
      " [ 1  4  0  0  0]]\n",
      "Accuracy:         0.76  -+0.0223\n",
      "(831, 32) (831,)\n",
      "Decision Tree [[51  1  3  6  0]\n",
      " [ 3 10  1  0  3]\n",
      " [ 1  0  1  1  1]\n",
      " [ 5  0  0  0  0]\n",
      " [ 0  2  0  0  2]]\n",
      "(830, 32) (830,)\n",
      "Decision Tree [[48  4  2  7  0]\n",
      " [ 1 14  0  0  2]\n",
      " [ 2  1  0  1  0]\n",
      " [ 3  1  1  0  0]\n",
      " [ 1  2  1  0  1]]\n",
      "(830, 32) (830,)\n",
      "Decision Tree [[52  3  3  3  0]\n",
      " [ 2 12  1  0  2]\n",
      " [ 2  0  1  1  0]\n",
      " [ 5  0  0  0  0]\n",
      " [ 0  3  0  0  2]]\n",
      "(830, 32) (830,)\n",
      "Decision Tree [[52  3  2  4  0]\n",
      " [ 3  9  1  1  3]\n",
      " [ 2  1  0  1  0]\n",
      " [ 3  0  1  1  0]\n",
      " [ 0  4  0  0  1]]\n",
      "(829, 32) (829,)\n",
      "Decision Tree [[51  1  1  8  0]\n",
      " [ 0 13  0  0  4]\n",
      " [ 3  0  0  1  1]\n",
      " [ 4  0  0  1  0]\n",
      " [ 0  3  0  0  2]]\n",
      "(831, 32) (831,)\n",
      "Decision Tree [[53  2  1  4  1]\n",
      " [ 3  7  4  0  3]\n",
      " [ 2  1  1  0  0]\n",
      " [ 5  0  0  0  0]\n",
      " [ 1  2  0  0  1]]\n",
      "(830, 32) (830,)\n",
      "Decision Tree [[57  1  0  3  0]\n",
      " [ 1 11  1  1  3]\n",
      " [ 0  0  1  3  0]\n",
      " [ 5  0  0  0  0]\n",
      " [ 1  3  0  0  1]]\n",
      "(830, 32) (830,)\n",
      "Decision Tree [[55  0  2  2  2]\n",
      " [ 3  9  0  0  5]\n",
      " [ 2  1  0  0  1]\n",
      " [ 4  0  0  1  0]\n",
      " [ 1  3  0  0  1]]\n",
      "(830, 32) (830,)\n",
      "Decision Tree [[51  2  3  5  0]\n",
      " [ 2 12  1  0  2]\n",
      " [ 1  0  1  2  0]\n",
      " [ 4  0  1  0  0]\n",
      " [ 2  3  0  0  0]]\n",
      "(827, 32) (827,)\n",
      "Decision Tree [[55  1  2  3  1]\n",
      " [ 2 15  0  0  0]\n",
      " [ 4  0  0  1  0]\n",
      " [ 6  0  0  0  0]\n",
      " [ 0  2  1  0  2]]\n",
      "Accuracy:         0.71  -+0.0208\n",
      "(831, 32) (831,)\n",
      "Support Vector Machine [[60  1  0  0  0]\n",
      " [ 4 13  0  0  0]\n",
      " [ 3  1  0  0  0]\n",
      " [ 5  0  0  0  0]\n",
      " [ 0  4  0  0  0]]\n",
      "(830, 32) (830,)\n",
      "Support Vector Machine [[58  3  0  0  0]\n",
      " [ 1 16  0  0  0]\n",
      " [ 3  1  0  0  0]\n",
      " [ 4  1  0  0  0]\n",
      " [ 0  5  0  0  0]]\n",
      "(830, 32) (830,)\n",
      "Support Vector Machine [[59  2  0  0  0]\n",
      " [ 2 15  0  0  0]\n",
      " [ 3  1  0  0  0]\n",
      " [ 5  0  0  0  0]\n",
      " [ 0  5  0  0  0]]\n",
      "(830, 32) (830,)\n",
      "Support Vector Machine [[55  6  0  0  0]\n",
      " [ 3 14  0  0  0]\n",
      " [ 2  2  0  0  0]\n",
      " [ 5  0  0  0  0]\n",
      " [ 1  4  0  0  0]]\n",
      "(829, 32) (829,)\n",
      "Support Vector Machine [[59  2  0  0  0]\n",
      " [ 1 16  0  0  0]\n",
      " [ 4  1  0  0  0]\n",
      " [ 4  1  0  0  0]\n",
      " [ 0  5  0  0  0]]\n",
      "(831, 32) (831,)\n",
      "Support Vector Machine [[58  3  0  0  0]\n",
      " [ 3 14  0  0  0]\n",
      " [ 1  3  0  0  0]\n",
      " [ 5  0  0  0  0]\n",
      " [ 0  4  0  0  0]]\n",
      "(830, 32) (830,)\n",
      "Support Vector Machine [[61  0  0  0  0]\n",
      " [ 1 16  0  0  0]\n",
      " [ 4  0  0  0  0]\n",
      " [ 5  0  0  0  0]\n",
      " [ 0  5  0  0  0]]\n",
      "(830, 32) (830,)\n",
      "Support Vector Machine [[55  6  0  0  0]\n",
      " [ 1 16  0  0  0]\n",
      " [ 2  2  0  0  0]\n",
      " [ 5  0  0  0  0]\n",
      " [ 0  5  0  0  0]]\n",
      "(830, 32) (830,)\n",
      "Support Vector Machine [[61  0  0  0  0]\n",
      " [ 1 16  0  0  0]\n",
      " [ 2  2  0  0  0]\n",
      " [ 5  0  0  0  0]\n",
      " [ 0  5  0  0  0]]\n",
      "(827, 32) (827,)\n",
      "Support Vector Machine [[59  3  0  0  0]\n",
      " [ 6 11  0  0  0]\n",
      " [ 5  0  0  0  0]\n",
      " [ 6  0  0  0  0]\n",
      " [ 0  5  0  0  0]]\n",
      "Accuracy:         0.79  -+0.0236\n",
      "(831, 32) (831,)\n",
      "Gradient Boosting [[60  1  0  0  0]\n",
      " [ 4 13  0  0  0]\n",
      " [ 1  0  2  0  1]\n",
      " [ 5  0  0  0  0]\n",
      " [ 1  3  0  0  0]]\n",
      "(830, 32) (830,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting [[55  2  4  0  0]\n",
      " [ 1 14  0  0  2]\n",
      " [ 2  1  1  0  0]\n",
      " [ 3  1  1  0  0]\n",
      " [ 1  4  0  0  0]]\n",
      "(830, 32) (830,)\n",
      "Gradient Boosting [[60  0  1  0  0]\n",
      " [ 3 13  0  0  1]\n",
      " [ 2  0  2  0  0]\n",
      " [ 5  0  0  0  0]\n",
      " [ 0  4  0  0  1]]\n",
      "(830, 32) (830,)\n",
      "Gradient Boosting [[58  3  0  0  0]\n",
      " [ 3 11  0  0  3]\n",
      " [ 3  0  1  0  0]\n",
      " [ 4  0  0  1  0]\n",
      " [ 0  4  1  0  0]]\n",
      "(829, 32) (829,)\n",
      "Gradient Boosting [[58  1  1  1  0]\n",
      " [ 0 15  0  0  2]\n",
      " [ 4  1  0  0  0]\n",
      " [ 5  0  0  0  0]\n",
      " [ 0  5  0  0  0]]\n",
      "(831, 32) (831,)\n",
      "Gradient Boosting [[57  2  1  1  0]\n",
      " [ 4 11  1  0  1]\n",
      " [ 1  2  0  0  1]\n",
      " [ 5  0  0  0  0]\n",
      " [ 0  3  0  0  1]]\n",
      "(830, 32) (830,)\n",
      "Gradient Boosting [[61  0  0  0  0]\n",
      " [ 1 15  0  0  1]\n",
      " [ 4  0  0  0  0]\n",
      " [ 5  0  0  0  0]\n",
      " [ 0  4  1  0  0]]\n",
      "(830, 32) (830,)\n",
      "Gradient Boosting [[56  4  0  0  1]\n",
      " [ 2 13  0  0  2]\n",
      " [ 2  1  0  0  1]\n",
      " [ 5  0  0  0  0]\n",
      " [ 0  4  0  0  1]]\n",
      "(830, 32) (830,)\n",
      "Gradient Boosting [[60  0  0  1  0]\n",
      " [ 2 15  0  0  0]\n",
      " [ 2  1  1  0  0]\n",
      " [ 5  0  0  0  0]\n",
      " [ 0  5  0  0  0]]\n",
      "(827, 32) (827,)\n",
      "Gradient Boosting [[60  1  0  0  1]\n",
      " [ 2 13  0  0  2]\n",
      " [ 5  0  0  0  0]\n",
      " [ 6  0  0  0  0]\n",
      " [ 1  3  0  0  1]]\n",
      "Accuracy:         0.79  -+0.0216\n",
      "(831, 32) (831,)\n",
      "VotingClassifier [[61  0  0  0  0]\n",
      " [ 4 13  0  0  0]\n",
      " [ 1  0  2  0  1]\n",
      " [ 5  0  0  0  0]\n",
      " [ 1  3  0  0  0]]\n",
      "(830, 32) (830,)\n",
      "VotingClassifier [[55  2  4  0  0]\n",
      " [ 1 14  0  0  2]\n",
      " [ 2  1  1  0  0]\n",
      " [ 3  1  1  0  0]\n",
      " [ 1  4  0  0  0]]\n",
      "(830, 32) (830,)\n",
      "VotingClassifier [[60  0  1  0  0]\n",
      " [ 2 14  0  0  1]\n",
      " [ 2  0  2  0  0]\n",
      " [ 5  0  0  0  0]\n",
      " [ 0  4  0  0  1]]\n",
      "(830, 32) (830,)\n",
      "VotingClassifier [[57  4  0  0  0]\n",
      " [ 3 11  0  0  3]\n",
      " [ 3  0  1  0  0]\n",
      " [ 5  0  0  0  0]\n",
      " [ 0  4  1  0  0]]\n",
      "(829, 32) (829,)\n",
      "VotingClassifier [[59  1  1  0  0]\n",
      " [ 1 14  0  0  2]\n",
      " [ 4  1  0  0  0]\n",
      " [ 5  0  0  0  0]\n",
      " [ 0  5  0  0  0]]\n",
      "(831, 32) (831,)\n",
      "VotingClassifier [[57  2  1  1  0]\n",
      " [ 4 11  1  0  1]\n",
      " [ 1  2  0  0  1]\n",
      " [ 5  0  0  0  0]\n",
      " [ 0  3  0  0  1]]\n",
      "(830, 32) (830,)\n",
      "VotingClassifier [[61  0  0  0  0]\n",
      " [ 1 15  0  0  1]\n",
      " [ 4  0  0  0  0]\n",
      " [ 5  0  0  0  0]\n",
      " [ 0  4  1  0  0]]\n",
      "(830, 32) (830,)\n",
      "VotingClassifier [[56  4  0  0  1]\n",
      " [ 2 13  0  0  2]\n",
      " [ 2  2  0  0  0]\n",
      " [ 5  0  0  0  0]\n",
      " [ 0  4  0  0  1]]\n",
      "(830, 32) (830,)\n",
      "VotingClassifier [[59  0  0  2  0]\n",
      " [ 2 15  0  0  0]\n",
      " [ 2  1  1  0  0]\n",
      " [ 5  0  0  0  0]\n",
      " [ 0  5  0  0  0]]\n",
      "(827, 32) (827,)\n",
      "VotingClassifier [[60  1  0  0  1]\n",
      " [ 2 12  0  0  3]\n",
      " [ 5  0  0  0  0]\n",
      " [ 6  0  0  0  0]\n",
      " [ 1  3  0  0  1]]\n",
      "Accuracy:         0.79  -+0.025\n"
     ]
    }
   ],
   "source": [
    "## 10 fold-CV\n",
    "\n",
    "for method in methods:\n",
    "    if method == 'Logistic Regression':\n",
    "        CL = LogisticRegression()\n",
    "    if method == 'K Neighbors Classifier':\n",
    "        CL = KNeighborsClassifier(n_neighbors=10)\n",
    "    if method == 'Random Forest':\n",
    "        CL = RandomForestClassifier(max_depth=35)\n",
    "    if method == 'Gaussian Naive Bayes':\n",
    "        CL = GaussianNB()\n",
    "    if method == 'Linear Discriminant Analysis':\n",
    "        CL = LinearDiscriminantAnalysis()\n",
    "    if method == 'Decision Tree':\n",
    "        CL = DecisionTreeClassifier()\n",
    "    if method == 'Support Vector Machine':\n",
    "        CL = SVC()\n",
    "    if method == 'Gradient Boosting':\n",
    "        CL = GradientBoostingClassifier()\n",
    "    if method == 'VotingClassifier':\n",
    "        cl1 = LogisticRegression()\n",
    "        cl2 = KNeighborsClassifier(n_neighbors=10)\n",
    "        cl3 = RandomForestClassifier(max_depth=35)\n",
    "        cl4 = GaussianNB()\n",
    "        cl5 = LinearDiscriminantAnalysis()\n",
    "        cl6 = DecisionTreeClassifier()\n",
    "        cl7 = SVC()\n",
    "        cl8 = GradientBoostingClassifier()\n",
    "        estimator = [(method[0],cl1), (method[1],cl2), (method[2],cl3), (method[3],cl4),\\\n",
    "                     (method[4],cl5), (method[5],cl6), (method[6],cl7), (method[7],cl8)]\n",
    "        eclf = VotingClassifier(estimators=estimator,\n",
    "        voting='soft', weights=[10, 8, 10, 10, 7, 5, 9, 9])\n",
    "\n",
    "    accList = []\n",
    "    for j in range(5):\n",
    "\n",
    "        i = j*.1\n",
    "        k = (j+1)*.1\n",
    "        #train\n",
    "        X_train = cat0.iloc[int(k * lenCat0):,:-1].append(\\\n",
    "                  cat1.iloc[int(k * lenCat1):,:-1].append(\\\n",
    "                  cat2.iloc[int(k * lenCat2):,:-1].append(\\\n",
    "                  cat5.iloc[int(k * lenCat5):,:-1].append(\\\n",
    "                  cat6.iloc[int(k * lenCat6):,:-1].append(\\\n",
    "                  cat0.iloc[:int(i * lenCat0),:-1].append(\\\n",
    "                  cat1.iloc[:int(i * lenCat1),:-1].append(\\\n",
    "                  cat2.iloc[:int(i * lenCat2),:-1].append(\\\n",
    "                  cat5.iloc[:int(i * lenCat5),:-1].append(\\\n",
    "                  cat6.iloc[:int(i * lenCat6),:-1]                                       \n",
    "                                                         )))))))))\n",
    "\n",
    "        y_train = cat0.iloc[int(k * lenCat0):,-1].append(\\\n",
    "                  cat1.iloc[int(k * lenCat1):,-1].append(\\\n",
    "                  cat2.iloc[int(k * lenCat2):,-1].append(\\\n",
    "                  cat5.iloc[int(k * lenCat5):,-1].append(\\\n",
    "                  cat6.iloc[int(k * lenCat6):,-1].append(\\\n",
    "                  cat0.iloc[:int(i * lenCat0),-1].append(\\\n",
    "                  cat1.iloc[:int(i * lenCat1),-1].append(\\\n",
    "                  cat2.iloc[:int(i * lenCat2),-1].append(\\\n",
    "                  cat5.iloc[:int(i * lenCat5),-1].append(\\\n",
    "                  cat6.iloc[:int(i * lenCat6),-1]                                         \n",
    "                                                        )))))))))\n",
    "#         print(X_train.shape, y_train.shape)\n",
    "    \n",
    "\n",
    "        # fit and apply the transform\n",
    "        X_over, y_over = oversample.fit_resample(X_train, y_train)\n",
    "\n",
    "        Max = dict(Counter(y_over))[0.0]\n",
    "        flag = True\n",
    "        while flag:\n",
    "            X_over, y_over = oversample.fit_resample(X_over, y_over)\n",
    "            for x in list(dict(Counter(y_over)).values()):\n",
    "                if x == Max:\n",
    "                    flag = False\n",
    "                else:\n",
    "                    flag = True\n",
    "                    break\n",
    "        \n",
    "        # summarize class distribution\n",
    "#         print(Counter(y_over))\n",
    "\n",
    "        #test\n",
    "        X_test = cat0.iloc[int(i * lenCat0):int(k * lenCat0),:-1].append(\\\n",
    "                 cat1.iloc[int(i * lenCat1):int(k * lenCat1),:-1].append(\\\n",
    "                 cat2.iloc[int(i * lenCat2):int(k * lenCat2),:-1].append(\\\n",
    "                 cat5.iloc[int(i * lenCat5):int(k * lenCat5),:-1].append(\\\n",
    "                 cat6.iloc[int(i * lenCat6):int(k * lenCat6),:-1]))))\n",
    "\n",
    "        y_test = cat0.iloc[int(i * lenCat0):int(k * lenCat0),-1].append(\\\n",
    "                 cat1.iloc[int(i * lenCat1):int(k * lenCat1),-1].append(\\\n",
    "                 cat2.iloc[int(i * lenCat2):int(k * lenCat2),-1].append(\\\n",
    "                 cat5.iloc[int(i * lenCat5):int(k * lenCat5),-1].append(\\\n",
    "                 cat6.iloc[int(i * lenCat6):int(k * lenCat6),-1]))))\n",
    "#         print(X_test.shape, y_test.shape)\n",
    "        #preprocessing\n",
    "        scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "        X_train_transformed = scaler.transform(X_train)\n",
    "        X_test_transformed = scaler.transform(X_test)\n",
    "        \n",
    "        model = CL.fit(X_train, np.array(list(y_train)))\n",
    "        \n",
    "        predict = np.array(model.predict(X_test))\n",
    "        \n",
    "        cnf_matrix = metrics.confusion_matrix(y_test, predict)\n",
    "        print(method)\n",
    "        print(cnf_matrix)\n",
    "#         #matplotlib inline\n",
    "#         class_names=[0,1] # name  of classes\n",
    "#         fig, ax = plt.subplots()\n",
    "#         tick_marks = np.arange(len(class_names))\n",
    "#         plt.xticks(tick_marks, class_names)\n",
    "#         plt.yticks(tick_marks, class_names)\n",
    "\n",
    "#         # create heatmap\n",
    "#         sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "#         ax.xaxis.set_label_position(\"top\")\n",
    "#         plt.tight_layout()\n",
    "#         plt.title(method+ ' fold of '+str(j), y=1.1)\n",
    "\n",
    "#         plt.ylabel('Actual label')\n",
    "#         plt.xlabel('Predicted label')\n",
    "\n",
    "#         plt.show()\n",
    "\n",
    "\n",
    "        acc = metrics.accuracy_score(y_test.values, predict)\n",
    "        accList.append(acc)\n",
    "\n",
    "\n",
    "    methodList.append(method)\n",
    "    confidence = 0.95\n",
    "\n",
    "    naccList = len(accList)\n",
    "    maccList = np.mean(accList)\n",
    "    std_erraccList = sem(accList)\n",
    "    haccList = std_erraccList * t.ppf((1 + confidence) / 2, naccList - 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Accuracy:         \"+ str(round(maccList,2)) + '  -+' + str(round(haccList,4)))\n",
    "    accRes.append(round(maccList,2))\n",
    "    accConf.append(round(haccList,4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>ACC CI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.0268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.0258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.0175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gaussian Naive Bayes</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.0325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.0223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.0237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.0236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.0233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>VotingClassifier</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.0268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.0258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Gaussian Naive Bayes</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.0325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.0223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.0175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.0236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.0241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.0268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.0258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.0142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Gaussian Naive Bayes</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.0325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.0223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.0236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.0216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>VotingClassifier</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.0250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Method  Accuracy  ACC CI\n",
       "0            Logistic Regression      0.82  0.0268\n",
       "1         K Neighbors Classifier      0.78  0.0258\n",
       "2                  Random Forest      0.81  0.0175\n",
       "3           Gaussian Naive Bayes      0.48  0.0325\n",
       "4   Linear Discriminant Analysis      0.76  0.0223\n",
       "5                  Decision Tree      0.70  0.0237\n",
       "6         Support Vector Machine      0.79  0.0236\n",
       "7              Gradient Boosting      0.79  0.0233\n",
       "8               VotingClassifier      0.80  0.0211\n",
       "9            Logistic Regression      0.82  0.0268\n",
       "10        K Neighbors Classifier      0.78  0.0258\n",
       "11                 Random Forest      0.80  0.0154\n",
       "12          Gaussian Naive Bayes      0.48  0.0325\n",
       "13  Linear Discriminant Analysis      0.76  0.0223\n",
       "14                 Decision Tree      0.70  0.0175\n",
       "15        Support Vector Machine      0.79  0.0236\n",
       "16             Gradient Boosting      0.79  0.0241\n",
       "17           Logistic Regression      0.82  0.0268\n",
       "18        K Neighbors Classifier      0.78  0.0258\n",
       "19                 Random Forest      0.81  0.0142\n",
       "20          Gaussian Naive Bayes      0.48  0.0325\n",
       "21  Linear Discriminant Analysis      0.76  0.0223\n",
       "22                 Decision Tree      0.71  0.0208\n",
       "23        Support Vector Machine      0.79  0.0236\n",
       "24             Gradient Boosting      0.79  0.0216\n",
       "25              VotingClassifier      0.79  0.0250"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = []\n",
    "result = pd.DataFrame(result)\n",
    "result['Method'],result['Accuracy'],result['ACC CI'] = methodList,accRes,accConf\n",
    "result.to_excel('kidney_Classification_Result.xlsx',index=False)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# methodList.append('Logistic Regression')\n",
    "# confidence = 0.95\n",
    "# # data = [1, 2, 3, 4, 5]\n",
    "\n",
    "# naccList = len(accList)\n",
    "# maccList = np.mean(accList)\n",
    "# std_erraccList = sem(accList)\n",
    "# haccList = std_erraccList * t.ppf((1 + confidence) / 2, naccList - 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"Accuracy:         \"+ str(round(maccList,2)) + '  -+' + str(round(haccList,4)))\n",
    "# accRes.append(round(maccList,2))\n",
    "# accConf.append(round(haccList,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loologreg = LogisticRegression()\n",
    "# loologregModel =loologreg.fit(X_train, np.array(list(y_train)))\n",
    "# predict = np.array(loologreg.predict(X_test))\n",
    "# cnf_matrix = metrics.confusion_matrix(y_test, predict)\n",
    "\n",
    "# print(cnf_matrix)\n",
    "\n",
    "# acc = metrics.accuracy_score(y_test.values, predict)\n",
    "# print(\"Logistic Regression Accuracy by LOOCV\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #matplotlib inline\n",
    "# class_names=[0,1] # name  of classes\n",
    "# fig, ax = plt.subplots()\n",
    "# tick_marks = np.arange(len(class_names))\n",
    "# plt.xticks(tick_marks, class_names)\n",
    "# plt.yticks(tick_marks, class_names)\n",
    "\n",
    "# # create heatmap\n",
    "# sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "# ax.xaxis.set_label_position(\"top\")\n",
    "# plt.tight_layout()\n",
    "# plt.title('Logestic Regression Confusion matrix', y=1.1)\n",
    "\n",
    "# plt.ylabel('Actual label')\n",
    "# plt.xlabel('Predicted label')\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# j = 1\n",
    "# i = j*.1\n",
    "# k = (j+1)*.1\n",
    "# #train\n",
    "# X_train = cat0.iloc[int(k * lenCat0):,:-1].append(\\\n",
    "#           cat1.iloc[int(k * lenCat1):,:-1].append(\\\n",
    "#           cat2.iloc[int(k * lenCat2):,:-1].append(\\\n",
    "#           cat5.iloc[int(k * lenCat5):,:-1].append(\\\n",
    "#           cat6.iloc[int(k * lenCat6):,:-1].append(\\\n",
    "#           cat0.iloc[:int(i * lenCat0),:-1].append(\\\n",
    "#           cat1.iloc[:int(i * lenCat1),:-1].append(\\\n",
    "#           cat2.iloc[:int(i * lenCat2),:-1].append(\\\n",
    "#           cat5.iloc[:int(i * lenCat5),:-1].append(\\\n",
    "#           cat6.iloc[:int(i * lenCat6),:-1]                                       \n",
    "#                                                  )))))))))\n",
    "\n",
    "# y_train = cat0.iloc[int(k * lenCat0):,-1].append(\\\n",
    "#           cat1.iloc[int(k * lenCat1):,-1].append(\\\n",
    "#           cat2.iloc[int(k * lenCat2):,-1].append(\\\n",
    "#           cat5.iloc[int(k * lenCat5):,-1].append(\\\n",
    "#           cat6.iloc[int(k * lenCat6):,-1].append(\\\n",
    "#           cat0.iloc[:int(i * lenCat0),-1].append(\\\n",
    "#           cat1.iloc[:int(i * lenCat1),-1].append(\\\n",
    "#           cat2.iloc[:int(i * lenCat2),-1].append(\\\n",
    "#           cat5.iloc[:int(i * lenCat5),-1].append(\\\n",
    "#           cat6.iloc[:int(i * lenCat6),-1]                                         \n",
    "#                                                 )))))))))\n",
    "# print(X_train.shape, y_train.shape)\n",
    "\n",
    "# #test\n",
    "# X_test = cat0.iloc[int(i * lenCat0):int(k * lenCat0),:-1].append(\\\n",
    "#          cat1.iloc[int(i * lenCat1):int(k * lenCat1),:-1].append(\\\n",
    "#          cat2.iloc[int(i * lenCat2):int(k * lenCat2),:-1].append(\\\n",
    "#          cat5.iloc[int(i * lenCat5):int(k * lenCat5),:-1].append(\\\n",
    "#          cat6.iloc[int(i * lenCat6):int(k * lenCat6),:-1]))))\n",
    "\n",
    "# y_test = cat0.iloc[int(i * lenCat0):int(k * lenCat0),-1].append(\\\n",
    "#          cat1.iloc[int(i * lenCat1):int(k * lenCat1),-1].append(\\\n",
    "#          cat2.iloc[int(i * lenCat2):int(k * lenCat2),-1].append(\\\n",
    "#          cat5.iloc[int(i * lenCat5):int(k * lenCat5),-1].append(\\\n",
    "#          cat6.iloc[int(i * lenCat6):int(k * lenCat6),-1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import datasets\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# # iris = datasets.load_iris()\n",
    "# # X, y = iris.data[:, 1:3], iris.target\n",
    "\n",
    "# clf1 = LogisticRegression(random_state=1)\n",
    "# clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n",
    "# clf3 = GaussianNB()\n",
    "\n",
    "# eclf = VotingClassifier(\n",
    "#     estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n",
    "#     voting='soft', weights=[3, 3, 1])\n",
    "\n",
    "# for clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Random Forest', 'naive Bayes', 'Ensemble']):\n",
    "#     scores = cross_val_score(clf, X_train, y_train, scoring='accuracy', cv=10)\n",
    "#     print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
