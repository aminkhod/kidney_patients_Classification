{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "from scipy.stats import sem, t\n",
    "from scipy import mean\n",
    "\n",
    "#Oversampling\n",
    "# from imblearn.over_sampling import RandomOverSampler\n",
    "from kmeans_smote import KMeansSMOTE\n",
    "\n",
    "\n",
    "### multiprocessing\n",
    "from multiprocessing.pool import Pool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(922, 33)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>SBP</th>\n",
       "      <th>BMI</th>\n",
       "      <th>etiology of CKD</th>\n",
       "      <th>Hb</th>\n",
       "      <th>Alb</th>\n",
       "      <th>Cr</th>\n",
       "      <th>eGFR</th>\n",
       "      <th>CKD_stage</th>\n",
       "      <th>...</th>\n",
       "      <th>eGFR(6M)</th>\n",
       "      <th>eGFR(12M)</th>\n",
       "      <th>eGFR(18M)</th>\n",
       "      <th>eGFR(24M)</th>\n",
       "      <th>eGFR(30M)</th>\n",
       "      <th>eGFR(36M)</th>\n",
       "      <th>eGFR(last visit)</th>\n",
       "      <th>average_obs</th>\n",
       "      <th>obsevasion_ duration</th>\n",
       "      <th>fclass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>74</td>\n",
       "      <td>120.0</td>\n",
       "      <td>23.137669</td>\n",
       "      <td>2</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.20</td>\n",
       "      <td>89.981926</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>26.454698</td>\n",
       "      <td>24.331582</td>\n",
       "      <td>24.682189</td>\n",
       "      <td>21.614854</td>\n",
       "      <td>20.420524</td>\n",
       "      <td>20.420524</td>\n",
       "      <td>18.495328</td>\n",
       "      <td>25.275139</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>139.0</td>\n",
       "      <td>28.515625</td>\n",
       "      <td>2</td>\n",
       "      <td>15.9</td>\n",
       "      <td>4.8</td>\n",
       "      <td>0.84</td>\n",
       "      <td>88.330020</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>78.287758</td>\n",
       "      <td>71.343858</td>\n",
       "      <td>72.845992</td>\n",
       "      <td>71.908942</td>\n",
       "      <td>71.562914</td>\n",
       "      <td>67.225032</td>\n",
       "      <td>67.225032</td>\n",
       "      <td>72.392152</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>154.0</td>\n",
       "      <td>24.582701</td>\n",
       "      <td>4</td>\n",
       "      <td>14.4</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0.87</td>\n",
       "      <td>86.973875</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>75.027238</td>\n",
       "      <td>69.595257</td>\n",
       "      <td>68.856399</td>\n",
       "      <td>72.901926</td>\n",
       "      <td>69.749275</td>\n",
       "      <td>69.171408</td>\n",
       "      <td>69.171408</td>\n",
       "      <td>72.694258</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>144.0</td>\n",
       "      <td>30.737407</td>\n",
       "      <td>2</td>\n",
       "      <td>14.4</td>\n",
       "      <td>4.7</td>\n",
       "      <td>2.22</td>\n",
       "      <td>86.874201</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>26.885061</td>\n",
       "      <td>24.917353</td>\n",
       "      <td>28.581660</td>\n",
       "      <td>29.237135</td>\n",
       "      <td>25.556002</td>\n",
       "      <td>25.183703</td>\n",
       "      <td>25.183703</td>\n",
       "      <td>26.485251</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>135.0</td>\n",
       "      <td>23.758726</td>\n",
       "      <td>2</td>\n",
       "      <td>17.0</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.39</td>\n",
       "      <td>86.782629</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>46.978867</td>\n",
       "      <td>45.829455</td>\n",
       "      <td>41.488436</td>\n",
       "      <td>41.801561</td>\n",
       "      <td>38.106104</td>\n",
       "      <td>38.106104</td>\n",
       "      <td>38.106104</td>\n",
       "      <td>43.081581</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender  age    SBP        BMI  etiology of CKD    Hb  Alb    Cr       eGFR  \\\n",
       "0       2   74  120.0  23.137669                2  12.0  4.0  1.20  89.981926   \n",
       "1       1   57  139.0  28.515625                2  15.9  4.8  0.84  88.330020   \n",
       "2       1   32  154.0  24.582701                4  14.4  4.4  0.87  86.973875   \n",
       "3       1   60  144.0  30.737407                2  14.4  4.7  2.22  86.874201   \n",
       "4       1   49  135.0  23.758726                2  17.0  4.1  1.39  86.782629   \n",
       "\n",
       "   CKD_stage  ...   eGFR(6M)  eGFR(12M)  eGFR(18M)  eGFR(24M)  eGFR(30M)  \\\n",
       "0          3  ...  26.454698  24.331582  24.682189  21.614854  20.420524   \n",
       "1          2  ...  78.287758  71.343858  72.845992  71.908942  71.562914   \n",
       "2          2  ...  75.027238  69.595257  68.856399  72.901926  69.749275   \n",
       "3          4  ...  26.885061  24.917353  28.581660  29.237135  25.556002   \n",
       "4          3  ...  46.978867  45.829455  41.488436  41.801561  38.106104   \n",
       "\n",
       "   eGFR(36M)  eGFR(last visit)  average_obs  obsevasion_ duration  fclass  \n",
       "0  20.420524         18.495328    25.275139                    37       0  \n",
       "1  67.225032         67.225032    72.392152                    37       0  \n",
       "2  69.171408         69.171408    72.694258                    36       0  \n",
       "3  25.183703         25.183703    26.485251                    35       0  \n",
       "4  38.106104         38.106104    43.081581                    30       0  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('prepared Data.csv')\n",
    "print(data.shape)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 611, 1: 170, 5: 51, 2: 42, 6: 48})"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = Counter(data['fclass'])\n",
    "count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# data = pd.read_csv(\"D://Blogs//train.csv\")\n",
    "# X = data.iloc[:,0:20]  #independent columns\n",
    "# y = data.iloc[:,-1]    #target column i.e price range\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "importances = dict()\n",
    "for col in data.iloc[:,:-1].columns:\n",
    "    importances[col] = 0\n",
    "\n",
    "for i in range(50):\n",
    "    model = ExtraTreesClassifier()\n",
    "    model.fit(data.iloc[:,:-1],data['fclass'])\n",
    "    matplotlib.rcParams['figure.figsize'] = (8.0, 6.0)\n",
    "    # print(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n",
    "    #plot graph of feature importances for better visualization\n",
    "    feat_importances = pd.Series(model.feature_importances_, index=data.iloc[:,:-1].columns)\n",
    "    for col in data.iloc[:,:-1].columns:\n",
    "        importances[col] += dict(feat_importances)[col]\n",
    "#     feat_importances.nlargest(5).plot(kind='barh')\n",
    "#     plt.show()\n",
    "    \n",
    "newpd = pd.DataFrame()\n",
    "for col in data.iloc[:,:-1].columns:\n",
    "    newpd[col] = list([importances[col]])\n",
    "newpd.to_csv('feature_importamce.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV \n",
    "  \n",
    "# defining parameter range for SVM\n",
    "param_grid = {'C': [0.1, 1, 10, 100, 1000],  \n",
    "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n",
    "              'kernel': ['poly','rbf','sigmoid']}  \n",
    "  \n",
    "grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3, cv=5) \n",
    "  \n",
    "# fitting the model for grid search \n",
    "max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "X_train = max_abs_scaler.fit_transform(data.values[:,:-1])\n",
    "grid.fit(X_train , data.values[:,-1])\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV \n",
    "  \n",
    "# # defining parameter range for SVM\n",
    "# param_grid = {'C': [0.1, 1, 10, 100, 1000],  \n",
    "#               'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n",
    "#               'kernel': ['poly','rbf','sigmoid']}  \n",
    "  \n",
    "# grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3) \n",
    "  \n",
    "# # fitting the model for grid search \n",
    "# grid.fit(data.values[:,:-1], data.values[:,-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV \n",
    "  \n",
    "# # defining parameter range for SVM\n",
    "# param_grid = {'C': [0.1, 1, 10, 100, 1000],  \n",
    "#               'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n",
    "#               'kernel': ['poly','rbf','sigmoid']}  \n",
    "  \n",
    "# grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3) \n",
    "  \n",
    "# # fitting the model for grid search \n",
    "# grid.fit(data.values[:,:-1], data.values[:,-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV \n",
    "  \n",
    "# # defining parameter range for SVM\n",
    "# param_grid = {'C': [0.1, 1, 10, 100, 1000],  \n",
    "#               'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n",
    "#               'kernel': ['poly','rbf','sigmoid']}  \n",
    "  \n",
    "# grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3) \n",
    "  \n",
    "# # fitting the model for grid search \n",
    "# grid.fit(data.values[:,:-1], data.values[:,-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV \n",
    "  \n",
    "# # defining parameter range for SVM\n",
    "# param_grid = {'C': [0.1, 1, 10, 100, 1000],  \n",
    "#               'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n",
    "#               'kernel': ['poly','rbf','sigmoid']}  \n",
    "  \n",
    "# grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3) \n",
    "  \n",
    "# # fitting the model for grid search \n",
    "# grid.fit(data.values[:,:-1], data.values[:,-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV \n",
    "  \n",
    "# # defining parameter range for SVM\n",
    "# param_grid = {'C': [0.1, 1, 10, 100, 1000],  \n",
    "#               'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n",
    "#               'kernel': ['poly','rbf','sigmoid']}  \n",
    "  \n",
    "# grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3) \n",
    "  \n",
    "# # fitting the model for grid search \n",
    "# grid.fit(data.values[:,:-1], data.values[:,-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV \n",
    "  \n",
    "# # defining parameter range for SVM\n",
    "# param_grid = {'C': [0.1, 1, 10, 100, 1000],  \n",
    "#               'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n",
    "#               'kernel': ['poly','rbf','sigmoid']}  \n",
    "  \n",
    "# grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3) \n",
    "  \n",
    "# # fitting the model for grid search \n",
    "# grid.fit(data.values[:,:-1], data.values[:,-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV \n",
    "  \n",
    "# # defining parameter range for SVM\n",
    "# param_grid = {'C': [0.1, 1, 10, 100, 1000],  \n",
    "#               'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n",
    "#               'kernel': ['poly','rbf','sigmoid']}  \n",
    "  \n",
    "# grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3) \n",
    "  \n",
    "# # fitting the model for grid search \n",
    "# grid.fit(data.values[:,:-1], data.values[:,-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV \n",
    "  \n",
    "# # defining parameter range for SVM\n",
    "# param_grid = {'C': [0.1, 1, 10, 100, 1000],  \n",
    "#               'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n",
    "#               'kernel': ['poly','rbf','sigmoid']}  \n",
    "  \n",
    "# grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3) \n",
    "  \n",
    "# # fitting the model for grid search \n",
    "# grid.fit(data.values[:,:-1], data.values[:,-1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y = data.values[:int(800),:-1], data.values[:int(800),-1]\n",
    "# [print('Class {} has {} instances'.format(label, count))\n",
    "#  for label, count in zip(*np.unique(y, return_counts=True))]\n",
    "\n",
    "# kmeans_smote = KMeansSMOTE(\n",
    "#     kmeans_args={\n",
    "#         'n_clusters': 63\n",
    "#     },\n",
    "#     smote_args={\n",
    "#         'k_neighbors': 10\n",
    "#     }\n",
    "# )\n",
    "# X_resampled, y_resampled = kmeans_smote.fit_sample(X, y)\n",
    "\n",
    "# [print('Class {} has {} instances after oversampling'.format(label, count))\n",
    "#  for label, count in zip(*np.unique(y_resampled, return_counts=True))]\n",
    "# print(Counter(y_resampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define oversampling strategy\n",
    "# oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "\n",
    "## Below code works for binary classification \n",
    "# oversample = RandomOverSampler(sampling_strategy=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # define oversampling strategy\n",
    "# oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "\n",
    "# # fit and apply the transform\n",
    "# X_over, y_over = oversample.fit_resample(data.values[:,:-1], data.values[:,-1])\n",
    "\n",
    "# Max = dict(Counter(y_over))[0.0]\n",
    "# flag = True\n",
    "# while flag:\n",
    "#     X_over, y_over = oversample.fit_resample(X_over, y_over)\n",
    "#     for x in list(dict(Counter(y_over)).values()):\n",
    "#         if x == Max:\n",
    "#             flag = False\n",
    "#         else:\n",
    "#             flag = True\n",
    "#             break\n",
    "        \n",
    "# # summarize class distribution\n",
    "# print(Counter(y_over))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adict = dict()\n",
    "# adict.update(zip(d.columns,col[:-1]))\n",
    "# adict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = pd.DataFrame(X_over)\n",
    "# adict = dict()\n",
    "# adict.update(zip(d.columns,col[:-1]))\n",
    "# d.rename(columns = adict, inplace =True)\n",
    "# d['fclass'] = y_over\n",
    "# d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = Counter(data['fclass'])\n",
    "# count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = d.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat0 = shuffle(data[data['fclass']==0])\n",
    "cat1 = shuffle(data[data['fclass']==1])\n",
    "cat2 = shuffle(data[data['fclass']==2])\n",
    "cat5 = shuffle(data[data['fclass']==5])\n",
    "cat6 = shuffle(data[data['fclass']==6])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lenCat0 = len(cat0.iloc[:,0])\n",
    "lenCat1 = len(cat1.iloc[:,0])\n",
    "lenCat2 = len(cat2.iloc[:,0])\n",
    "lenCat5 = len(cat5.iloc[:,0])\n",
    "lenCat6 = len(cat6.iloc[:,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methodList = []\n",
    "accRes = []\n",
    "accConf = []\n",
    "nFold = 10\n",
    "# sensitivityList = []\n",
    "# specificityList = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "basicMethods = ['Gradient Boosting','Support Vector Machine', 'Logistic Regression', 'K Neighbors Classifier',\\\n",
    "                'Random Forest', 'Gaussian Naive Bayes', 'Linear Discriminant Analysis', 'Decision Tree']\n",
    "\n",
    "methods = ['stacking classifir', 'soft votingClassifier','hard votingClassifier', 'Gradient Boosting',\\\n",
    "           'Support Vector Machine', 'Logistic Regression', 'K Neighbors Classifier', 'Random Forest',\\\n",
    "           'Gaussian Naive Bayes', 'Linear Discriminant Analysis', 'Decision Tree']\n",
    "# methods = ['hard votingClassifier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## 10 fold-CV\n",
    "\n",
    "def classifiers(method):\n",
    "    cl1 = LogisticRegression(solver='lbfgs', max_iter=500)\n",
    "    cl2 = KNeighborsClassifier(n_neighbors=10)\n",
    "    cl3 = RandomForestClassifier(max_depth=50)\n",
    "    cl4 = GaussianNB()\n",
    "    cl5 = LinearDiscriminantAnalysis()\n",
    "    cl6 = DecisionTreeClassifier()\n",
    "    cl7 = SVC(C=0.1, gamma=0.0001, kernel='poly',probability=True)\n",
    "    cl8 = GradientBoostingClassifier()\n",
    "    estimator = [(basicMethods[0],cl1), (basicMethods[1],cl2), (basicMethods[2],cl3),\\\n",
    "                 (basicMethods[3],cl4), (basicMethods[4],cl5), (basicMethods[5],cl6),\\\n",
    "                 (basicMethods[6],cl7), (basicMethods[7],cl8)]\n",
    "    \n",
    "    if method == 'Logistic Regression':\n",
    "        CL = LogisticRegression(solver='lbfgs', max_iter=500)\n",
    "        \n",
    "    if method == 'K Neighbors Classifier':\n",
    "        CL = KNeighborsClassifier(n_neighbors=10)\n",
    "        \n",
    "    if method == 'Random Forest':\n",
    "        CL = RandomForestClassifier(max_depth=50)\n",
    "        \n",
    "    if method == 'Gaussian Naive Bayes':\n",
    "        CL = GaussianNB()\n",
    "        \n",
    "    if method == 'Linear Discriminant Analysis':\n",
    "        CL = LinearDiscriminantAnalysis()\n",
    "        \n",
    "    if method == 'Decision Tree':\n",
    "        CL = DecisionTreeClassifier()\n",
    "        \n",
    "    if method == 'Support Vector Machine':\n",
    "        CL = SVC(C=1, gamma=0.0001, kernel='poly',probability=True)\n",
    "#         class_weight='balanced', C=1.0\n",
    "\n",
    "    if method == 'Gradient Boosting':\n",
    "        CL = GradientBoostingClassifier()\n",
    "        \n",
    "    if method == 'soft votingClassifier':\n",
    "        CL = VotingClassifier(estimators=estimator, voting='soft', weights=[10, 3, 6, 5, 10, 5, 6, 8])\n",
    "        \n",
    "    if method == 'hard votingClassifier':\n",
    "        CL = VotingClassifier(estimators=estimator, voting='hard')\n",
    "    \n",
    "    if method == 'stacking classifir':\n",
    "        CL = StackingClassifier(estimators=estimator, cv=5)\n",
    "    print(type(CL))\n",
    "    accList = []\n",
    "    for j in range(nFold):\n",
    "\n",
    "        i = j*.1\n",
    "        k = (j+1)*.1\n",
    "        #train\n",
    "        X_train = cat0.iloc[int(k * lenCat0):,:-1].append(\\\n",
    "                  cat1.iloc[int(k * lenCat1):,:-1].append(\\\n",
    "                  cat2.iloc[int(k * lenCat2):,:-1].append(\\\n",
    "                  cat5.iloc[int(k * lenCat5):,:-1].append(\\\n",
    "                  cat6.iloc[int(k * lenCat6):,:-1].append(\\\n",
    "                  cat0.iloc[:int(i * lenCat0),:-1].append(\\\n",
    "                  cat1.iloc[:int(i * lenCat1),:-1].append(\\\n",
    "                  cat2.iloc[:int(i * lenCat2),:-1].append(\\\n",
    "                  cat5.iloc[:int(i * lenCat5),:-1].append(\\\n",
    "                  cat6.iloc[:int(i * lenCat6),:-1]                                       \n",
    "                                                         )))))))))\n",
    "\n",
    "        y_train = cat0.iloc[int(k * lenCat0):,-1].append(\\\n",
    "                  cat1.iloc[int(k * lenCat1):,-1].append(\\\n",
    "                  cat2.iloc[int(k * lenCat2):,-1].append(\\\n",
    "                  cat5.iloc[int(k * lenCat5):,-1].append(\\\n",
    "                  cat6.iloc[int(k * lenCat6):,-1].append(\\\n",
    "                  cat0.iloc[:int(i * lenCat0),-1].append(\\\n",
    "                  cat1.iloc[:int(i * lenCat1),-1].append(\\\n",
    "                  cat2.iloc[:int(i * lenCat2),-1].append(\\\n",
    "                  cat5.iloc[:int(i * lenCat5),-1].append(\\\n",
    "                  cat6.iloc[:int(i * lenCat6),-1]                                         \n",
    "                                                        )))))))))\n",
    "#         print(X_train.shape, y_train.shape)\n",
    "\n",
    "\n",
    "        #preprocessing\n",
    "        max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "        X_train = max_abs_scaler.fit_transform(X_train)\n",
    "        \n",
    "        scaler = preprocessing.StandardScaler()\n",
    "#         X_train = scaler.fit_transform(X_train)\n",
    " \n",
    "\n",
    "        # fit and apply the Oversampling:\n",
    "        #### Repeatitive oversampling\n",
    "#         X_over, y_over = oversample.fit_resample(X_train, y_train)\n",
    "\n",
    "#         Max = dict(Counter(y_over))[0.0]\n",
    "#         flag = True\n",
    "#         while flag:\n",
    "#             X_over, y_over = oversample.fit_resample(X_over, y_over)\n",
    "#             for x in list(dict(Counter(y_over)).values()):\n",
    "#                 if x == Max:\n",
    "#                     flag = False\n",
    "#                 else:\n",
    "#                     flag = True\n",
    "#                     break\n",
    "#         X_train, y_train = X_over.copy(), y_over.copy()\n",
    "        # summarize class distribution\n",
    "#         print(Counter(y_train))\n",
    "\n",
    "#         print(X_train.shape, y_train.shape)\n",
    "    \n",
    "\n",
    "        # fit and apply the Oversampling:\n",
    "        #### K-means oversampling\n",
    "#         [print('Class {} has {} instances'.format(label, count))\n",
    "#          for label, count in zip(*np.unique(y, return_counts=True))]\n",
    "\n",
    "        kmeans_smote = KMeansSMOTE(\n",
    "            kmeans_args={\n",
    "                'n_clusters': 63\n",
    "            },\n",
    "            smote_args={\n",
    "                'k_neighbors': 10\n",
    "            }\n",
    "        )\n",
    "        X_train, y_train = kmeans_smote.fit_sample(X_train, y_train)\n",
    "\n",
    "#         [print('Class {} has {} instances after oversampling'.format(label, count))\n",
    "#          for label, count in zip(*np.unique(y_resampled, return_counts=True))]\n",
    "        # summarize class distribution\n",
    "#         print(Counter(y_train))\n",
    "\n",
    "        #test\n",
    "        X_test = cat0.iloc[int(i * lenCat0):int(k * lenCat0),:-1].append(\\\n",
    "                 cat1.iloc[int(i * lenCat1):int(k * lenCat1),:-1].append(\\\n",
    "                 cat2.iloc[int(i * lenCat2):int(k * lenCat2),:-1].append(\\\n",
    "                 cat5.iloc[int(i * lenCat5):int(k * lenCat5),:-1].append(\\\n",
    "                 cat6.iloc[int(i * lenCat6):int(k * lenCat6),:-1]))))\n",
    "\n",
    "        y_test = cat0.iloc[int(i * lenCat0):int(k * lenCat0),-1].append(\\\n",
    "                 cat1.iloc[int(i * lenCat1):int(k * lenCat1),-1].append(\\\n",
    "                 cat2.iloc[int(i * lenCat2):int(k * lenCat2),-1].append(\\\n",
    "                 cat5.iloc[int(i * lenCat5):int(k * lenCat5),-1].append(\\\n",
    "                 cat6.iloc[int(i * lenCat6):int(k * lenCat6),-1]))))\n",
    "#         print(X_test.shape, y_test.shape)\n",
    "        X_test = max_abs_scaler.transform(X_test)\n",
    "#         X_test = scaler.transform(X_test)   \n",
    "        \n",
    "        model = CL.fit(X_train, np.array(list(y_train)))\n",
    "        \n",
    "        predict = np.array(model.predict(X_test))\n",
    "        \n",
    "        cnf_matrix = metrics.confusion_matrix(y_test, predict)\n",
    "        print(method)\n",
    "        print(cnf_matrix)\n",
    "#         #matplotlib inline\n",
    "#         class_names=[0,1] # name  of classes\n",
    "#         fig, ax = plt.subplots()\n",
    "#         tick_marks = np.arange(len(class_names))\n",
    "#         plt.xticks(tick_marks, class_names)\n",
    "#         plt.yticks(tick_marks, class_names)\n",
    "\n",
    "#         # create heatmap\n",
    "#         sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "#         ax.xaxis.set_label_position(\"top\")\n",
    "#         plt.tight_layout()\n",
    "#         plt.title(method+ ' fold of '+str(j), y=1.1)\n",
    "\n",
    "#         plt.ylabel('Actual label')\n",
    "#         plt.xlabel('Predicted label')\n",
    "\n",
    "#         plt.show()\n",
    "\n",
    "\n",
    "        acc = metrics.accuracy_score(y_test.values, predict)\n",
    "        accList.append(acc)\n",
    "\n",
    "\n",
    "    methodList.append(method)\n",
    "    confidence = 0.95\n",
    "\n",
    "    naccList = len(accList)\n",
    "    maccList = np.mean(accList)\n",
    "    std_erraccList = sem(accList)\n",
    "    haccList = std_erraccList * t.ppf((1 + confidence) / 2, naccList - 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Accuracy:         \"+ str(round(maccList,2)) + '  -+' + str(round(haccList,4)))\n",
    "    accRes.append(round(maccList,2))\n",
    "    accConf.append(round(haccList,4))\n",
    "    return (method, round(maccList,2), round(haccList,4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    p = Pool(processes=7)\n",
    "    result1 = p.map(classifiers, methods)\n",
    "    p.close()\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(result1)\n",
    "result.columns = ['Method','Accuracy','ACC CI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # result = []\n",
    "# result = pd.DataFrame(result1)\n",
    "# result['Method'],result['Accuracy'],result['ACC CI'] = methodList,accRes,accConf\n",
    "result.to_excel('kidney_Classification_Result_preprocess before oversampling.xlsx',index=False)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
